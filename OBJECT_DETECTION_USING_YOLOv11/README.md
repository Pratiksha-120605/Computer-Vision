ğŸ” Custom Object Detection using YOLOv11 on Rock-Paper-Scissors Dataset
I implemented a custom object detection model using YOLOv11, trained on the Rock Paper Scissors dataset. The goal was to accurately detect hand gestures corresponding to rock, paper, and scissors in real-time images.

ğŸ§  Key Features:
Trained a YOLOv11 model using transfer learning.

Achieved high accuracy in detecting and classifying hand gestures.

Used bounding boxes to localize each gesture in the image.

ğŸ“‚ Dataset:
Dataset: https://universe.roboflow.com/roboflow-58fyf/rock-paper-scissors-sxsw

Contains labeled images of hand gestures under varying backgrounds and lighting conditions.

ğŸ”§ Tech Stack:
Python

YOLOv11 (Ultralytics fork or custom implementation)

PyTorch

OpenCV

Roboflow
